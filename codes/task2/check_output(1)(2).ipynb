{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git (from -r /Users/aryanmathur/Desktop/41_m3_adobe/41_code_m3_adobe/task2/requirements21.txt (line 1))\n",
      "  Cloning https://github.com/huggingface/transformers.git to /private/var/folders/zv/2m5dbz0j7xlc8h11nms8zm100000gn/T/pip-req-build-ph89xf4w\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /private/var/folders/zv/2m5dbz0j7xlc8h11nms8zm100000gn/T/pip-req-build-ph89xf4w\n",
      "  error: RPC failed; curl 56 LibreSSL SSL_read: LibreSSL/3.3.6: error:1404C3FC:SSL routines:ST_OK:sslv3 alert bad record mac, errno 0\n",
      "  fatal: expected 'packfile'\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet \u001b[0m\u001b[4;32mhttps://github.com/huggingface/transformers.git\u001b[0m\u001b[32m \u001b[0m\u001b[32m/private/var/folders/zv/2m5dbz0j7xlc8h11nms8zm100000gn/T/\u001b[0m\u001b[32mpip-req-build-ph89xf4w\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet \u001b[0m\u001b[4;32mhttps://github.com/huggingface/transformers.git\u001b[0m\u001b[32m \u001b[0m\u001b[32m/private/var/folders/zv/2m5dbz0j7xlc8h11nms8zm100000gn/T/\u001b[0m\u001b[32mpip-req-build-ph89xf4w\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r /Users/aryanmathur/Desktop/41_m3_adobe/41_code_m3_adobe/task2/requirements(2)(1).txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import clip\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    # Delete variables if they exist in the current global scope\n",
    "    if \"inputs\" in globals():\n",
    "        del globals()[\"inputs\"]\n",
    "    if \"model\" in globals():\n",
    "        del globals()[\"model\"]\n",
    "    if \"processor\" in globals():\n",
    "        del globals()[\"processor\"]\n",
    "    if \"trainer\" in globals():\n",
    "        del globals()[\"trainer\"]\n",
    "    if \"peft_model\" in globals():\n",
    "        del globals()[\"peft_model\"]\n",
    "    if \"bnb_config\" in globals():\n",
    "        del globals()[\"bnb_config\"]\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Garbage collection and clearing CUDA memory\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(2)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available on this system.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Supports bfloat16: {torch.cuda.is_bf16_supported()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this system.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear_memory() uncomment for cuda to clear gpu memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8ff29195df4c6cb9cdba9c76d135db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "processor = Qwen2VLProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(img_path):\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device)\n",
    "\n",
    "    geometric_and_structural_anomalies = [\n",
    "        \"Inconsistent object boundaries\",\n",
    "        \"Discontinuous surfaces\",\n",
    "        \"Non-manifold geometries in rigid structures\",\n",
    "        \"Floating or disconnected components\",\n",
    "        \"Asymmetric features in naturally symmetric objects\",\n",
    "        \"Misaligned bilateral elements in animal faces\",\n",
    "        \"Irregular proportions in mechanical components\",\n",
    "        \"Impossible mechanical connections\",\n",
    "        \"Inconsistent scale of mechanical parts\",\n",
    "        \"Physically impossible structural elements\",\n",
    "        \"Incorrect wheel geometry\",\n",
    "        \"Implausible aerodynamic structures\",\n",
    "        \"Misaligned body panels\",\n",
    "        \"Impossible mechanical joints\",\n",
    "        \"Anatomically impossible joint configurations\",\n",
    "        \"Unnatural pose artifacts\",\n",
    "        \"Biological asymmetry errors\",\n",
    "        \"Excessive sharpness in certain image regions\",\n",
    "        \"Unnaturally glossy surfaces\"\n",
    "    ]\n",
    "\n",
    "    texture_and_surface_issues = [\n",
    "        \"Texture bleeding between adjacent regions\",\n",
    "        \"Texture repetition patterns\",\n",
    "        \"Over-smoothing of natural textures\",\n",
    "        \"Artificial noise patterns in uniform surfaces\",\n",
    "        \"Metallic surface artifacts\",\n",
    "        \"Artificial enhancement artifacts\",\n",
    "        \"Regular grid-like artifacts in textures\",\n",
    "        \"Repeated element patterns\",\n",
    "        \"Synthetic material appearance\",\n",
    "        \"Artificial smoothness\"\n",
    "    ]\n",
    "\n",
    "    lighting_and_reflection_problems = [\n",
    "        \"Unrealistic specular highlights\",\n",
    "        \"Inconsistent material properties\",\n",
    "        \"Multiple light source conflicts\",\n",
    "        \"Missing ambient occlusion\",\n",
    "        \"Incorrect reflection mapping\",\n",
    "        \"Inconsistent shadow directions\",\n",
    "        \"Glow or light bleed around object boundaries\",\n",
    "        \"Incorrect Skin Tones\",\n",
    "        \"Unnatural Lighting Gradients\",\n",
    "        \"Dramatic lighting that defies natural physics\",\n",
    "        \"Multiple inconsistent shadow sources\"\n",
    "    ]\n",
    "\n",
    "    anatomical_and_biological_anomalies = [\n",
    "        \"Dental anomalies in mammals\",\n",
    "        \"Anatomically incorrect paw structures\",\n",
    "        \"Improper fur direction flows\",\n",
    "        \"Unrealistic eye reflections\",\n",
    "        \"Misshapen ears or appendages\",\n",
    "        \"Anatomically impossible joint configurations\",\n",
    "        \"Impossible foreshortening in animal bodies\",\n",
    "        \"Exaggerated characteristic features\"\n",
    "    ]\n",
    "\n",
    "    perspective_and_spatial_distortions = [\n",
    "        \"Incorrect perspective rendering\",\n",
    "        \"Scale inconsistencies within single objects\",\n",
    "        \"Spatial relationship errors\",\n",
    "        \"Depth perception anomalies\",\n",
    "        \"Fake depth of field\",\n",
    "        \"Resolution inconsistencies within regions\",\n",
    "        \"Artificial depth of field in object presentation\",\n",
    "        \"Impossible mechanical joints\"\n",
    "    ]\n",
    "\n",
    "    image_quality_issues = [\n",
    "        \"Over-sharpening artifacts\",\n",
    "        \"Aliasing along high-contrast edges\",\n",
    "        \"Blurred boundaries in fine details\",\n",
    "        \"Jagged edges in curved structures\",\n",
    "        \"Random noise patterns in detailed areas\",\n",
    "        \"Loss of fine detail in complex structures\",\n",
    "        \"Systematic color distribution anomalies\",\n",
    "        \"Color coherence breaks\",\n",
    "        \"Unnatural color transitions\",\n",
    "        \"Frequency domain signatures\"\n",
    "    ]\n",
    "\n",
    "    visual_artifacts_from_synthetic_image_generation = [\n",
    "        \"Ghosting effects: Semi-transparent duplicates of elements\",\n",
    "        \"Cinematization Effects\",\n",
    "        \"Movie-poster like composition of ordinary scenes\",\n",
    "        \"Unnatural pose artifacts\"\n",
    "    ]\n",
    "\n",
    "    occlusion_and_object_cutoff_issues = [\n",
    "        \"Abruptly cut off objects\",\n",
    "        \"Inconsistent object boundaries\"\n",
    "    ]\n",
    "\n",
    "    artifact_groups = {\n",
    "        \"Geometric and Structural Anomalies\": geometric_and_structural_anomalies,\n",
    "        \"Texture and Surface Issues\" : texture_and_surface_issues,\n",
    "        \"Lighting and Reflection Problems\" : lighting_and_reflection_problems,\n",
    "        \"Anatomical and Biological Anomalies\": anatomical_and_biological_anomalies,\n",
    "        \"Perspective and Spatial Distortions\" : perspective_and_spatial_distortions,\n",
    "        \"Image Quality Issues\" : image_quality_issues,\n",
    "        \"Visual Artifacts from Synthetic Image Generation\": visual_artifacts_from_synthetic_image_generation,\n",
    "        \"Occlusion and Object Cut-off Issues\": occlusion_and_object_cutoff_issues\n",
    "    }\n",
    "\n",
    "\n",
    "    img = Image.open(img_path)\n",
    "    img_input = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "    text_inputs = torch.cat([clip.tokenize(f\"A photo of {artifact}\") for artifact in artifact_groups]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(img_input)\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    similarity = (image_features @ text_features.T).squeeze(0)\n",
    "\n",
    "    artifact_group_names = list(artifact_groups.keys())\n",
    "\n",
    "    top3_indices = similarity.topk(3).indices\n",
    "    top3_scores = similarity[top3_indices].tolist()\n",
    "    top3_categories = [artifact_group_names[idx] for idx in top3_indices]\n",
    "\n",
    "    l = []\n",
    "    for category in top3_categories:\n",
    "        l.extend(artifact_groups[category])\n",
    "\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(l):\n",
    "    random.shuffle(l)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Evaluate all artifact categories listed below and select MOST RELEVANT artifacts present in the image. Rank the detected artifacts in descending order of their intensity or confidence and provide an concise and image-specific explanation for each detected artifact in the following format:\n",
    "\n",
    "<Artifact Category 1>: <Explanation for selection>\n",
    "<Artifact Category 2>: <Explanation for selection>\n",
    "...\n",
    "\n",
    "Word Limit for the total explanation is 50 words\n",
    "\n",
    "Unordered list of Artifact Categories (order does not imply priority):\n",
    "{chr(10).join(f\"{category}\" for category in l)}\n",
    "\n",
    "Your role is to evaluate **all categories equally**, without prioritizing based on their order in the list, and rank them based on their intensity or confidence. Provide accurate and reliable artifact detection results in the **Answer Format** described above.\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"What artifacts can you detect in the provided image?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message(\n",
    "    image_path,\n",
    "    question=question,\n",
    "):\n",
    "    \n",
    "    entry = [\n",
    "                {\n",
    "                    \"role\":\"system\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\", \n",
    "                            \"text\": create_prompt(filter(img_path= image_path)),\n",
    "                        },\n",
    "                    ],\n",
    "                },\n",
    "                {\n",
    "                    \"role\":\"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                            \"image\":  image_path,\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": question,\n",
    "                        },\n",
    "                    ],\n",
    "                },\n",
    "            ]\n",
    "    return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def output(message): \n",
    "\n",
    "    text = processor.apply_chat_template(\n",
    "        message, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    image_inputs, _ = process_vision_info(message)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=150)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    \n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fin_output(\n",
    "    image_dir=\"/teamspace/studios/this_studio/fake\", \n",
    "    output_file=\"./testing_fake.json\"\n",
    "):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        results = []\n",
    "        for i in sorted(os.listdir(image_dir)):\n",
    "            image_path = os.path.join(image_dir, i)\n",
    "            if i.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "                file_name = i.split(\".\")[0]\n",
    "\n",
    "                message = create_message(image_path=image_path)\n",
    "                output_text = output(message=message)\n",
    "\n",
    "                try:\n",
    "                    lines = [line.strip() for line in output_text[0].split(\"\\n\") if line.strip()]\n",
    "                    \n",
    "                    output_dict = {}\n",
    "                    for line in lines:\n",
    "                        parts = line.split(\":\", 1)\n",
    "                        if len(parts) == 2:\n",
    "                            key = parts[0].strip()\n",
    "                            value = parts[1].strip()\n",
    "                            \n",
    "                            if len(value.split()) > 80:\n",
    "                                print(f\"Skipping key '{key}' in {image_path} - word count exceeds 50\")\n",
    "                                continue\n",
    "                            \n",
    "                            output_dict[key] = value\n",
    "\n",
    "                    json_data = {\"explanation\": output_dict}\n",
    "\n",
    "                    final_data = {\n",
    "                        \"index\": file_name,\n",
    "                        **json_data\n",
    "                    }\n",
    "                    print(final_data)\n",
    "\n",
    "                    results.append(final_data)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_path}: {e}\")\n",
    "                    results.append({\n",
    "                        \"index\": file_name,\n",
    "                        \"error\": str(e)\n",
    "                    })\n",
    "\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
